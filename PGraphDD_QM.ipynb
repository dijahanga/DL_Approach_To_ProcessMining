{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PGraphDD-QM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dijahanga/DL_Approach_To_Process_Mining/blob/main/PGraphDD_QM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75RLiZ_hzKMz"
      },
      "source": [
        "import pandas as pd\n",
        "import graphviz\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.layers import Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import pydotplus as pydot\n",
        "from graphviz import Digraph\n",
        "import copy\n",
        "import csv\n",
        "from google.colab import files\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oomav8Y_QBm"
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4EIOW39tTg"
      },
      "source": [
        "class dataDivider:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.tracesPerPart = 0\n",
        "    self.totalCount = 0\n",
        "  def getPartDictionary(self, labels):\n",
        "    indexes = []\n",
        "    _activeCase = None\n",
        "    caseLabel = labels.case;\n",
        "    indexi = []\n",
        "    index = 0\n",
        "    for index, row in self.data.iterrows():\n",
        "      if (_activeCase == None):\n",
        "        _activeCase = row[caseLabel]\n",
        "        indexi = [index, -1]\n",
        "      else:\n",
        "        if (_activeCase != row[caseLabel]):\n",
        "          indexi[1] = index\n",
        "          indexes.append(tuple(indexi))\n",
        "          indexi[0] = index + 1\n",
        "          _activeCase = row[caseLabel]\n",
        "    indexi[1] = index + 1\n",
        "    indexes.append(tuple(indexi))\n",
        "    return indexes\n",
        "  def setParts(self, labels, parts):\n",
        "    indexes = self.getPartDictionary(labels)\n",
        "    if parts > len(indexes):\n",
        "      raise ValueError('Part cannot be greater than total events') \n",
        "    approxSize = round(len(indexes)/parts)\n",
        "    partIndexes = []\n",
        "    partCount = []\n",
        "    for i in range(0, parts):\n",
        "      top = i * approxSize\n",
        "      bottom = top + approxSize -1\n",
        "      startIndex = indexes[top][0]\n",
        "      if len(indexes) <= bottom:\n",
        "        bottom = len(indexes) - 1\n",
        "      endIndex = indexes[bottom][1];\n",
        "      partIndexes.append([startIndex, endIndex])\n",
        "      partCount.append(approxSize)\n",
        "    self.partIndexes = partIndexes\n",
        "    self.totalCount = len(indexes)\n",
        "    self.tracesPerPart = approxSize\n",
        "    return partIndexes\n",
        "  def getPartIndex(self, indx):\n",
        "    return self.data.iloc[self.partIndexes[indx][0]], self.data.iloc[self.partIndexes[indx][1]];\n",
        "  def getPart(self, index):\n",
        "    return self.data.iloc[self.partIndexes[index][0]:self.partIndexes[index][1], :]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxfTRcBj2PHN"
      },
      "source": [
        "class prepareData:\n",
        "  def __init__(self, data, label):\n",
        "    self.data = data\n",
        "    self.label = label\n",
        "  def create_input_output(self, xy):\n",
        "    # Define Empty List\n",
        "    values = []\n",
        "    xList = []\n",
        "    _ncols = ('X', 'Y')\n",
        "    values.append((\"NULL\", xy[0]))\n",
        "    i = 0\n",
        "    while i < len(xy):\n",
        "        try:\n",
        "            xList = xy[0: i+1]\n",
        "            xList.insert(0, \"NULL\")\n",
        "            values.append((xList, xy[i + 1]))\n",
        "        except:\n",
        "            xList = xy[0: i+1]\n",
        "            xList.insert(0, \"NULL\")\n",
        "            values.append((xList, \"END\"))\n",
        "        i = i + 1\n",
        "    return pd.DataFrame(values, columns=_ncols) \n",
        "\n",
        "  def prepare(self, validEvts = None, test_size = 0, tokenizer = None):\n",
        "    nameLabel = self.label[0]\n",
        "    valueLabel = self.label[1]\n",
        "    _activeCase = \"NULL\"\n",
        "    _tempxy = []\n",
        "    _ncols = ('X', 'Y')\n",
        "    maindfObj = pd.DataFrame([], columns=_ncols)\n",
        "    if validEvts is not None:\n",
        "      helperObj = helper()\n",
        "      validEvts = helperObj.oneDimStrToLower(validEvts)\n",
        "    for index, row in self.data.iterrows():\n",
        "      if validEvts is not None and row[valueLabel].lower() not in validEvts:\n",
        "        continue\n",
        "      if nameLabel in row and (row[nameLabel] == _activeCase or _activeCase == \"NULL\"):\n",
        "        concatenatedString = row[valueLabel]\n",
        "        _tempxy.append(concatenatedString)\n",
        "        _activeCase = row[nameLabel]\n",
        "      else:\n",
        "        subObject = self.create_input_output(_tempxy)\n",
        "        maindfObj = maindfObj.append(subObject)\n",
        "        _activeCase = row[nameLabel]\n",
        "        _tempxy.clear()\n",
        "        concatenatedString = row[valueLabel]\n",
        "        _tempxy.append(concatenatedString)\n",
        "    self.tokenize(maindfObj, tokenizer)\n",
        "    self.maindfObj = maindfObj\n",
        "    return self.custom_split(self.X, self.Y, test_size)\n",
        "\n",
        "  def append_to_2d(self, former_2d, new_2d):\n",
        "    for i in range(len(new_2d)):\n",
        "      former_2d.append(new_2d[i])\n",
        "    return former_2d\n",
        "\n",
        "  def custom_split(self, X, Y, test_size):\n",
        "    Xtrain = []\n",
        "    Ytrain = []\n",
        "    Xtest = []\n",
        "    Ytest = []\n",
        "    size = X.shape  \n",
        "    import random\n",
        "    startList = []\n",
        "    endList = []\n",
        "    for i in range(size[0]):\n",
        "      consid = X[i]\n",
        "      if consid[len(consid) - 2] == 0:\n",
        "        startList.append(i)\n",
        "        if(i > 0):\n",
        "          endList.append(i-1)\n",
        "    endList.append(size[0]-1) #Tail End of the Array is the last element of endList\n",
        "    num_test = int(round(len(startList)*test_size))  \n",
        "    num_train = len(startList) - num_test    \n",
        "    t = random.sample(startList, num_test)\n",
        "    counter = 0\n",
        "    for i in startList:\n",
        "      Xcase = np.array(X[i:endList[counter]+1])\n",
        "      Ycase = np.array(Y[i:endList[counter]+1])\n",
        "      if (i in t):\n",
        "        Xtest = self.append_to_2d(Xtest, Xcase)\n",
        "        Ytest = self.append_to_2d(Ytest, Ycase)\n",
        "      else:\n",
        "        Xtrain = self.append_to_2d(Xtrain, Xcase)\n",
        "        Ytrain = self.append_to_2d(Ytrain, Ycase)\n",
        "      counter = counter + 1\n",
        "    return np.array(Xtrain), np.array(Xtest), np.array(Ytrain), np.array(Ytest)\n",
        "\n",
        "  def tokenize(self, data, tokenizer):\n",
        "    if tokenizer is None:\n",
        "      tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tokenizer.fit_on_texts(data['X'])\n",
        "    X = tokenizer.texts_to_sequences(data['X'])\n",
        "    word_index = tokenizer.word_index\n",
        "    print(word_index)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    X = pad_sequences(X)\n",
        "    Y = pd.get_dummies(data['Y'])\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.tokenizer = tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LfY-qkVT1X"
      },
      "source": [
        "class helper:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def datasetListMergeMinus(self, dataset, subset):\n",
        "    wholeData = 1\n",
        "    for m in dataset:\n",
        "      if not m.equals(subset):\n",
        "        if type(wholeData) is int:\n",
        "          wholeData = m          \n",
        "        else:\n",
        "          wholeData = wholeData.append(m)\n",
        "    return wholeData\n",
        "  def multiDimStrToUpper(self, string):\n",
        "    nstring = []\n",
        "    for strns in string:\n",
        "      nstring.append([x.upper() for x in strns])\n",
        "    return nstring\n",
        "  def multiDimStrToLower(self, string):\n",
        "    nstring = []\n",
        "    for strns in string:\n",
        "      nstring.append([x.lower() for x in strns])\n",
        "    return nstring\n",
        "  def oneDimStrToLower(self, string):\n",
        "    nstring = []\n",
        "    for i in range(0, len(string)):\n",
        "      nstring.append(string[i].lower())\n",
        "    return nstring\n",
        "  def grabEventsFromHeader(self, header):\n",
        "    evs = []\n",
        "    c = 0\n",
        "    for ev in header:\n",
        "      if c > 0:\n",
        "        try:\n",
        "          num = int(ev)\n",
        "        except:\n",
        "          evs.append(ev)\n",
        "      c = c + 1\n",
        "    return evs\n",
        "\n",
        "  def rowIsFirst(self, row, activities, headers):\n",
        "    foundValues = []\n",
        "    for i in range(0,len(headers)):\n",
        "      val = headers[i]\n",
        "      if row[val] in activities:\n",
        "        foundValues.append(row[val])\n",
        "    if (len(foundValues) == 0):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def rowIsLast(self, row, evName):\n",
        "    rowEv = row.idxmax()\n",
        "    try:\n",
        "      if rowEv.lower() == evName.lower():\n",
        "        return True\n",
        "    except:\n",
        "      return False\n",
        "    return False\n",
        "\n",
        "  def divideMatrix(self, matrix):\n",
        "    headers = list(matrix.columns.values)\n",
        "    leftHeaders = []\n",
        "    rightHeaders = []\n",
        "    for i in range(0,len(headers)):\n",
        "      ev = headers[i]\n",
        "      try:\n",
        "          num = int(ev)        \n",
        "          leftHeaders.append(ev)        \n",
        "      except:\n",
        "          rightHeaders.append(ev)\n",
        "    leftData = matrix[leftHeaders]\n",
        "    rightData = matrix[rightHeaders]\n",
        "    return [leftData, rightData]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTeO1DxWVbA_"
      },
      "source": [
        "class training:\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    MAX_NB_WORDS =20   #50\n",
        "    EMBEDDING_DIM =20   #32\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    self.model = model\n",
        "  def train(self, tindx = '0'):\n",
        "    model = self.model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    print('Training...')\n",
        "    X = self.X\n",
        "    Y = self.Y\n",
        "    history = model.fit(X, Y,  epochs=50, batch_size=250, verbose=0)\n",
        "    self.model = model\n",
        "    model.save('Orig_model_'+tindx+'.h5')\n",
        "    return model.evaluate(X, Y)\n",
        "\n",
        "  def getModelFrom(self, modelprefix):\n",
        "    try:\n",
        "      modelName = 'Orig_model_'+modelprefix+'.h5';\n",
        "      self.model = load_model(modelName)\n",
        "    except:\n",
        "      self.train(modelprefix)\n",
        "\n",
        "  def align(self, from_, to_):\n",
        "    originalColumnNamesArr = to_.columns.values\n",
        "    driftedColumnNamesArr = from_.columns.values\n",
        "    colNum = []\n",
        "    for i in range(0, len(driftedColumnNamesArr)):\n",
        "      col = driftedColumnNamesArr[i]\n",
        "      if(col not in originalColumnNamesArr):\n",
        "        from_ = from_.drop(col, 1)\n",
        "        colNum.append(i)\n",
        "    for i in range(0, len(originalColumnNamesArr)):\n",
        "      col = originalColumnNamesArr[i]\n",
        "      if(col not in driftedColumnNamesArr):\n",
        "        from_[col] = 0\n",
        "    return from_, colNum\n",
        "\n",
        "  def validateModel(self, Prep_data, tokenizer, model,  X, Y):\n",
        "    predict_proba = model.predict(X)\n",
        "    colName = []\n",
        "    for i in Y:\n",
        "        colName.append(i)\n",
        "    dfObj = pd.DataFrame(list(np.round(predict_proba*100, decimals=0)), columns = colName)\n",
        "    Seq_Series=Prep_data.X.apply(pd.Series)\n",
        "    dfObj.reset_index(drop=True, inplace=True)\n",
        "    Seq_Series.reset_index(drop=True, inplace=True)\n",
        "    df_new = pd.concat([Seq_Series, dfObj], axis=1)\n",
        "    return df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_kWHt7DVjf2"
      },
      "source": [
        "class resultGraphing:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def decomposeResult(self, results):\n",
        "    helper_ = helper()\n",
        "    headers = list(results.columns.values)\n",
        "    events = helper_.grabEventsFromHeader(headers)\n",
        "    matrices = []\n",
        "    matricesLeft = []\n",
        "    lastIndex = 0\n",
        "    totalBegin = 0\n",
        "    ind = 0\n",
        "    newMatrix = helper_.divideMatrix(results)\n",
        "    for index, row in newMatrix[1].iterrows():\n",
        "      ind = index\n",
        "      rowIsLast = helper_.rowIsLast(row, \"end\")\n",
        "      if rowIsLast:\n",
        "        sequenceList = newMatrix[1].iloc[lastIndex:index+1, :]\n",
        "        sequenceListLeft = newMatrix[0].iloc[lastIndex:index+1, :]\n",
        "        matrices.append(sequenceList)\n",
        "        matricesLeft.append(sequenceListLeft)\n",
        "        lastIndex = index + 1\n",
        "    return matrices, matricesLeft\n",
        "\n",
        "  def rowIsAnewSequence(self, row):\n",
        "    try:\n",
        "      return math.isnan(row[\"1\"])\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  def linkAndProbabilities(self, matrices, count = 0):\n",
        "    links = []\n",
        "    probabilities = []\n",
        "    sequences = []\n",
        "    uniqueEvs = []\n",
        "    for i in range(0,len(matrices)):\n",
        "      thisMatrix = matrices[i]\n",
        "      lastEvent = \"Start\"\n",
        "      sequence = []\n",
        "      for index, row in thisMatrix.iterrows():\n",
        "        row = pd.to_numeric(row)\n",
        "        #print(row) # prints the rows\n",
        "        evName = row.idxmax(axis=1) # picks event with the highest probability\n",
        "        link = lastEvent + \"<-->\" + evName\n",
        "        sequence.append(evName)\n",
        "        if link not in links:\n",
        "          if lastEvent != evName:\n",
        "            if not (lastEvent.lower() == \"start\" and evName.lower() == \"end\"):\n",
        "              links.append(link)\n",
        "              prob = row[evName]\n",
        "              probabilities.append(prob)\n",
        "        lastEvent = evName\n",
        "      # The Last element is End and undesirable\n",
        "      sequence.pop()\n",
        "      sequences.append(sequence)\n",
        "    return links, probabilities, sequences\n",
        "\n",
        "  def drawGraph(self, transitions, counter):\n",
        "    G = Digraph('process_model', filename='dum_'+str(counter)+'.gv')\n",
        "    G.attr(rankdir='LR', size='7,5')\n",
        "    G.attr('node', shape='doublecircle', style=\"filled\", fillcolor=\"grey\")\n",
        "    G.node('Start')\n",
        "    G.node('END')\n",
        "    G.attr('node', shape='box', style=\"bold\")\n",
        "    for i in range(0,len(transitions)):\n",
        "      G.attr('edge', style=\"bold\", penwidth='3.0')\n",
        "      fromto = transitions[i].split(\"<-->\")\n",
        "      G.edge(fromto[0], fromto[1])\n",
        "    G.view()\n",
        "    return G\n",
        "  \n",
        "  def getEventSequence(self, data, X_label, Y_label):\n",
        "    currentX_label = ''\n",
        "    sequences = []\n",
        "    sequence = []\n",
        "    for index, row in data.iterrows():\n",
        "      if currentX_label == row[X_label]:\n",
        "        sequence.append(row[Y_label])\n",
        "      else:\n",
        "        if len(sequence) > 1:\n",
        "          sequences.append(sequence)\n",
        "        sequence = []\n",
        "        sequence.append(row[Y_label])\n",
        "      currentX_label = row[X_label]\n",
        "    sequences.append(sequence)\n",
        "    return sequences  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-oDgrnWdOT"
      },
      "source": [
        "class performance:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "    \n",
        "  def fitness(self, holdOut, sequences):\n",
        "    TruePositives = 0\n",
        "    Count = 0\n",
        "    searched = []\n",
        "    for i in range(0,len(holdOut)):\n",
        "      found = holdOut[i] in sequences      \n",
        "      alreadySearched = holdOut[i] in searched\n",
        "      searched.append(holdOut[i])\n",
        "      if alreadySearched:\n",
        "        Count = Count + 1\n",
        "      #else:\n",
        "        #count = count + 1\n",
        "      if found:\n",
        "        if alreadySearched:\n",
        "          TruePositives = TruePositives + 1\n",
        "        else:\n",
        "          TruePositives = TruePositives + 1\n",
        "\n",
        "    print(\"The fitness of the discovered model against the holdout part\")\n",
        "    print(\" No. of True Positive: \" , TruePositives)\n",
        "    print(\" No. of Traces in holdout: \", len(holdOut))\n",
        "    return (TruePositives/len(holdOut))\n",
        "\n",
        "  def precision(self, original, sequences):\n",
        "    TruePositives = 0\n",
        "    Count = 0\n",
        "    searched = []\n",
        "    for i in range(0,len(original)):\n",
        "      found = original[i] in sequences\n",
        "      alreadySearched = original[i] in searched\n",
        "      searched.append(original[i])\n",
        "      if alreadySearched:\n",
        "        Count = Count + 1\n",
        "      if found:\n",
        "        if alreadySearched:\n",
        "          TruePositives = TruePositives + 1\n",
        "        else:\n",
        "          TruePositives = TruePositives + 1\n",
        "\n",
        "    print(\"The precision of the discovered model against the complete log\")\n",
        "    print(\" No. of True Positive: \", TruePositives)\n",
        "    print(\"No. of Traces in the model: \", len(sequences))\n",
        "    return (TruePositives/len(original))\n",
        "\n",
        "  def findFScore(self, fitness, precision):\n",
        "    a = fitness\n",
        "    b = precision\n",
        "    return (2 * (a * b)/(a + b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfj-84QQ8AWW"
      },
      "source": [
        "class DatasetDefinitions:\n",
        "  def getUnique(self, label, dataset):\n",
        "    chains = []\n",
        "    chainTag = []\n",
        "    lastActivity = None\n",
        "    lastCase = None\n",
        "    eventLabel = label.event\n",
        "    caseLabel = label.case\n",
        "    for index, row in dataset.iterrows():\n",
        "      if lastCase is None or lastCase != row[caseLabel]:\n",
        "        lastActivity = None\n",
        "      if lastActivity is None:\n",
        "        lastActivity = row[eventLabel]\n",
        "        lastCase = row[caseLabel]\n",
        "        continue\n",
        "      lastCase = row[caseLabel]\n",
        "      if lastActivity != row[eventLabel]:\n",
        "        evChain = lastActivity +\"\"+row[eventLabel]\n",
        "        evChain = evChain.lower().strip()\n",
        "        evChain = \" \".join(evChain.split()).replace(' ', '_')\n",
        "        if evChain not in chains:\n",
        "          chains.append(evChain)\n",
        "          chainTag.append(lastCase)\n",
        "      lastActivity = row[eventLabel]\n",
        "    return chains, chainTag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIvcgnWU9g5P"
      },
      "source": [
        "class FindDrift:\n",
        "  def __init__(self, baseModel):\n",
        "    self.baseModel = baseModel\n",
        "  def executeAgainst(self, dataset, label):\n",
        "    _datasetDefinitions = DatasetDefinitions()\n",
        "    chain_base, tag_base = _datasetDefinitions.getUnique(label, self.baseModel)\n",
        "    chain, tag = _datasetDefinitions.getUnique(label, dataset)\n",
        "    indx = self.getDrifts(chain_base, chain)\n",
        "    tag_list = [tag[i] for i in indx]\n",
        "    chain_list = [chain[i] for i in indx]\n",
        "    return tag_list, chain_list, indx\n",
        "  def getDrifts(self, chain_base, chain_drift):\n",
        "    indx = []\n",
        "    counter = 0\n",
        "    for c in chain_drift:\n",
        "      if(c not in chain_base):\n",
        "        indx.append(counter)\n",
        "      counter = counter + 1\n",
        "    return indx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd6SRiRQWexE"
      },
      "source": [
        "labels = namedtuple(\"labels\", \"case event\")\n",
        "label = labels('case', 'event')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgRoSu7tWssb"
      },
      "source": [
        "parts = 20\n",
        "dataset = pd.read_csv('roi-2500.csv', low_memory= False)\n",
        "data_divider = dataDivider(dataset)\n",
        "p_ = data_divider.setParts(label, parts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HZ0FmTWBGsB"
      },
      "source": [
        "fScoreLog = []\n",
        "DriftLog = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm9qNPQaW3SV"
      },
      "source": [
        "for i in range(0, parts):\n",
        "  j = i + 1\n",
        "  if(i == parts - 1):\n",
        "    j = 0\n",
        "  #Get the log part to be used\n",
        "  referenceLog = data_divider.getPart(i)\n",
        "  detectionLog = data_divider.getPart(j)\n",
        "\n",
        "  #Prepare the data Reference\n",
        "  prepdata = prepareData(referenceLog, ['case', 'event'])\n",
        "  X_train, X_test, Y_train, Y_test = prepdata.prepare(None, 0)\n",
        "\n",
        "  #get built tokenizer and word_index\n",
        "  tokenizer = prepdata.tokenizer\n",
        "  X = prepdata.X\n",
        "  Y = prepdata.Y\n",
        "  v = tokenizer.word_index.keys()\n",
        "\n",
        "  #Prepare the data Detection\n",
        "  prepdata_t = prepareData(detectionLog, ['case', 'event'])\n",
        "  X_train_t, X_test_t, Y_train_t, Y_test_t = prepdata_t.prepare(list(v), 0, tokenizer) #list(v)  \n",
        "  tokenizer_t = prepdata_t.tokenizer\n",
        "  X_t = prepdata_t.X\n",
        "  Y_t = prepdata_t.Y\n",
        "\n",
        "  #Train the Reference Model\n",
        "  trainModel = training(X_train, Y_train)\n",
        "  trainModel.getModelFrom(str(i))\n",
        "\n",
        "  while True:\n",
        "    diff = - X.shape[1] + X_t.shape[1]\n",
        "    if(diff > 0):\n",
        "      X_t = np.delete(X_t, np.s_[0:1], axis=1)\n",
        "      print(\"Reshaped RD. difference was \"+str(diff)+\" \")\n",
        "    elif(diff < 0):\n",
        "      X_t = np.insert(X_t, 0, [0], axis=1)\n",
        "    else:\n",
        "      break\n",
        "  \n",
        "  #Use on itself\n",
        "  resultDataset_o = trainModel.validateModel(prepdata.maindfObj, tokenizer, trainModel.model,  X, Y)\n",
        "  #Use on Detection log\n",
        "  resultDataset_t = trainModel.validateModel(prepdata_t.maindfObj, tokenizer_t, trainModel.model,  X_t, Y)\n",
        "  resultGraphing_ = resultGraphing()\n",
        "\n",
        "  #Reference Log\n",
        "  probs_o, seqs_o = resultGraphing_.decomposeResult(resultDataset_o)\n",
        "  link_o, probabilities_o, sequences_o = resultGraphing_.linkAndProbabilities(probs_o)\n",
        "\n",
        "  #Reference Log\n",
        "  probs_t, seqs_t = resultGraphing_.decomposeResult(resultDataset_t)\n",
        "  #Detection Log\n",
        "  link_t, probabilities_t, sequences_t = resultGraphing_.linkAndProbabilities(probs_t)\n",
        "\n",
        "  #Calculate performance\n",
        "  performance_ = performance()\n",
        "  fitness = performance_.fitness(sequences_o, sequences_t)\n",
        "  precision = performance_.precision(sequences_o, sequences_t)\n",
        "  try:\n",
        "    fScore = performance_.findFScore(fitness, precision)\n",
        "  except:\n",
        "    fScore = 0\n",
        "  if fScore < 0.9:\n",
        "    DriftLog.append(\"Drift found in Window \"+str(j))\n",
        "  ra = (i, j, fScore)\n",
        "  fScoreLog.append(ra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaF2UUajWSHQ"
      },
      "source": [
        "print(sequences_o)\n",
        "print(sequences_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz31Y_megNJB"
      },
      "source": [
        "class windowManager:\n",
        "  def trainWindow(self, parts):\n",
        "    for i in range(0, parts):\n",
        "      j = i + 1\n",
        "      if(i == parts - 1):\n",
        "        j = 0\n",
        "      #Get the log part to be used\n",
        "      referenceLog = data_divider.getPart(i)\n",
        "      detectionLog = data_divider.getPart(j)\n",
        "\n",
        "      #Prepare the data Reference\n",
        "      prepdata = prepareData(referenceLog, ['case', 'event'])\n",
        "      X_train, X_test, Y_train, Y_test = prepdata.prepare(None, 0)\n",
        "\n",
        "      #get built tokenizer and word_index\n",
        "      tokenizer = prepdata.tokenizer\n",
        "      X = prepdata.X\n",
        "      Y = prepdata.Y\n",
        "      v = tokenizer.word_index.keys()\n",
        "\n",
        "      #Prepare the data Detection\n",
        "      prepdata_t = prepareData(detectionLog, ['case', 'event'])\n",
        "      X_train_t, X_test_t, Y_train_t, Y_test_t = prepdata_t.prepare(list(v), 0, tokenizer) #list(v)  \n",
        "      tokenizer_t = prepdata_t.tokenizer\n",
        "      X_t = prepdata_t.X\n",
        "      Y_t = prepdata_t.Y\n",
        "\n",
        "      #Train the Reference Model\n",
        "      trainModel = training(X_train, Y_train)\n",
        "      trainModel.getModelFrom(str(i))\n",
        "\n",
        "      while True:\n",
        "        diff = - X.shape[1] + X_t.shape[1]\n",
        "        if(diff > 0):\n",
        "          X_t = np.delete(X_t, np.s_[0:1], axis=1)\n",
        "          print(\"Reshaped RD. difference was \"+str(diff)+\" \")\n",
        "        elif(diff < 0):\n",
        "          X_t = np.insert(X_t, 0, [0], axis=1)\n",
        "        else:\n",
        "          break\n",
        "      \n",
        "      #Use on itself\n",
        "      resultDataset_o = trainModel.validateModel(prepdata.maindfObj, tokenizer, trainModel.model,  X, Y)\n",
        "      #Use on Detection log\n",
        "      resultDataset_t = trainModel.validateModel(prepdata_t.maindfObj, tokenizer_t, trainModel.model,  X_t, Y)\n",
        "      resultGraphing_ = resultGraphing()\n",
        "\n",
        "      #Reference Log\n",
        "      probs_o, seqs_o = resultGraphing_.decomposeResult(resultDataset_o)\n",
        "      link_o, probabilities_o, sequences_o = resultGraphing_.linkAndProbabilities(probs_o)\n",
        "\n",
        "      #Reference Log\n",
        "      probs_t, seqs_t = resultGraphing_.decomposeResult(resultDataset_t)\n",
        "      #Detection Log\n",
        "      link_t, probabilities_t, sequences_t = resultGraphing_.linkAndProbabilities(probs_t)\n",
        "\n",
        "      #Calculate performance\n",
        "      performance_ = performance()\n",
        "      fitness = performance_.fitness(sequences_o, sequences_t)\n",
        "      precision = performance_.precision(sequences_o, sequences_t)\n",
        "\n",
        "      fScore = performance_.findFScore(fitness, precision);\n",
        "      if fScore < 0.8:\n",
        "        DriftLog.append(\"Drift found in Window \"+str(j))\n",
        "      ra = (i, j, fScore)\n",
        "      fScoreLog.append(ra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBR3ba2ukkvU"
      },
      "source": [
        "_windowManager = windowManager()\n",
        "_windowManager.trainWindow(parts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cAxZxE3YN7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd5e6da-9232-4054-863f-f3c04466b416"
      },
      "source": [
        "  DriftLog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Drift found in Window 2',\n",
              " 'Drift found in Window 6',\n",
              " 'Drift found in Window 10',\n",
              " 'Drift found in Window 14',\n",
              " 'Drift found in Window 18']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKIWEtz9qi6L"
      },
      "source": [
        "fScoreLog"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}