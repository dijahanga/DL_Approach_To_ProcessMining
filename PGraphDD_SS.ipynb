{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PGraphDD-SS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dijahanga/DL_Approach_To_Process_Mining/blob/main/PGraphDD_SS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75RLiZ_hzKMz"
      },
      "source": [
        "import pandas as pd\n",
        "import graphviz\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.layers import Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import pydotplus as pydot\n",
        "from graphviz import Digraph\n",
        "import copy\n",
        "import csv\n",
        "from google.colab import files\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4EIOW39tTg"
      },
      "source": [
        "class dataDivider:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.tracesPerPart = 0\n",
        "    self.totalCount = 0\n",
        "  def getPartDictionary(self, labels):\n",
        "    indexes = []\n",
        "    _activeCase = None\n",
        "    caseLabel = labels.case;\n",
        "    indexi = []\n",
        "    index = 0\n",
        "    for index, row in self.data.iterrows():\n",
        "      if (_activeCase == None):\n",
        "        _activeCase = row[caseLabel]\n",
        "        indexi = [index, -1]\n",
        "      else:\n",
        "        if (_activeCase != row[caseLabel]):\n",
        "          indexi[1] = index\n",
        "          indexes.append(tuple(indexi))\n",
        "          indexi[0] = index + 1\n",
        "          _activeCase = row[caseLabel]\n",
        "    indexi[1] = index + 1\n",
        "    indexes.append(tuple(indexi))\n",
        "    return indexes\n",
        "  def setParts(self, labels, parts):\n",
        "    indexes = self.getPartDictionary(labels)\n",
        "    if parts > len(indexes):\n",
        "      raise ValueError('Part cannot be greater than total events') \n",
        "    approxSize = round(len(indexes)/parts)\n",
        "    partIndexes = []\n",
        "    partIndexesSeries = []\n",
        "    partCount = []\n",
        "    for i in range(0, parts):\n",
        "      top = i * approxSize\n",
        "      bottom = top + approxSize -1\n",
        "      startIndex = indexes[top][0]\n",
        "      if len(indexes) <= bottom:\n",
        "        bottom = len(indexes) - 1\n",
        "      endIndex = indexes[bottom][1];\n",
        "      series = indexes[top:bottom+1]      \n",
        "      partIndexes.append([startIndex, endIndex])\n",
        "      partIndexesSeries.append(series)\n",
        "      partCount.append(approxSize)\n",
        "    self.partIndexes = partIndexes\n",
        "    self.totalCount = len(indexes)\n",
        "    self.tracesPerPart = approxSize\n",
        "    self.partIndexesSeries = partIndexesSeries\n",
        "    return partIndexes\n",
        "  def getPartIndex(self, indx):\n",
        "    return self.data.iloc[self.partIndexes[indx][0]], self.data.iloc[self.partIndexes[indx][1]];\n",
        "  def getPart(self, index, percentage = 100):\n",
        "    if(percentage == 100):\n",
        "      return self.data.iloc[self.partIndexes[index][0]:self.partIndexes[index][1], :]\n",
        "    thisSeries = self.partIndexesSeries[index]\n",
        "    count = round((percentage/100) * len(thisSeries));\n",
        "    topIndex = thisSeries[0][0];\n",
        "    bottomIndex = thisSeries[count+1][1]\n",
        "    return self.data.iloc[topIndex:bottomIndex, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxfTRcBj2PHN"
      },
      "source": [
        "class prepareData:\n",
        "  def __init__(self, data, label):\n",
        "    self.data = data\n",
        "    self.label = label\n",
        "  def create_input_output(self, xy):\n",
        "    # Define Empty List\n",
        "    values = []\n",
        "    xList = []\n",
        "    _ncols = ('X', 'Y')\n",
        "    values.append((\"NULL\", xy[0]))\n",
        "    i = 0\n",
        "    while i < len(xy):\n",
        "        try:\n",
        "            xList = xy[0: i+1]\n",
        "            xList.insert(0, \"NULL\")\n",
        "            values.append((xList, xy[i + 1]))\n",
        "        except:\n",
        "            xList = xy[0: i+1]\n",
        "            xList.insert(0, \"NULL\")\n",
        "            values.append((xList, \"END\"))\n",
        "        i = i + 1\n",
        "    return pd.DataFrame(values, columns=_ncols) \n",
        "\n",
        "  def prepare(self, validEvts = None, test_size = 0, tokenizer = None):\n",
        "    nameLabel = self.label[0]\n",
        "    valueLabel = self.label[1]\n",
        "    _activeCase = \"NULL\"\n",
        "    _tempxy = []\n",
        "    _ncols = ('X', 'Y')\n",
        "    maindfObj = pd.DataFrame([], columns=_ncols)\n",
        "    if validEvts is not None:\n",
        "      helperObj = helper()\n",
        "      validEvts = helperObj.oneDimStrToLower(validEvts)\n",
        "    for index, row in self.data.iterrows():\n",
        "      if validEvts is not None and row[valueLabel].lower() not in validEvts:\n",
        "        continue\n",
        "      if nameLabel in row and (row[nameLabel] == _activeCase or _activeCase == \"NULL\"):\n",
        "        concatenatedString = row[valueLabel]\n",
        "        _tempxy.append(concatenatedString)\n",
        "        _activeCase = row[nameLabel]\n",
        "      else:\n",
        "        subObject = self.create_input_output(_tempxy)\n",
        "        maindfObj = maindfObj.append(subObject)\n",
        "        _activeCase = row[nameLabel]\n",
        "        _tempxy.clear()\n",
        "        concatenatedString = row[valueLabel]\n",
        "        _tempxy.append(concatenatedString)\n",
        "    self.tokenize(maindfObj, tokenizer)\n",
        "    self.maindfObj = maindfObj\n",
        "    return self.custom_split(self.X, self.Y, test_size)\n",
        "\n",
        "  def append_to_2d(self, former_2d, new_2d):\n",
        "    for i in range(len(new_2d)):\n",
        "      former_2d.append(new_2d[i])\n",
        "    return former_2d\n",
        "\n",
        "  def custom_split(self, X, Y, test_size):\n",
        "    Xtrain = []\n",
        "    Ytrain = []\n",
        "    Xtest = []\n",
        "    Ytest = []\n",
        "    size = X.shape  \n",
        "    import random\n",
        "    startList = []\n",
        "    endList = []\n",
        "    for i in range(size[0]):\n",
        "      consid = X[i]\n",
        "      if consid[len(consid) - 2] == 0:\n",
        "        startList.append(i)\n",
        "        if(i > 0):\n",
        "          endList.append(i-1)\n",
        "    endList.append(size[0]-1) #Tail End of the Array is the last element of endList\n",
        "    num_test = int(round(len(startList)*test_size))  \n",
        "    num_train = len(startList) - num_test    \n",
        "    t = random.sample(startList, num_test)\n",
        "    counter = 0\n",
        "    for i in startList:\n",
        "      Xcase = np.array(X[i:endList[counter]+1])\n",
        "      Ycase = np.array(Y[i:endList[counter]+1])\n",
        "      if (i in t):\n",
        "        Xtest = self.append_to_2d(Xtest, Xcase)\n",
        "        Ytest = self.append_to_2d(Ytest, Ycase)\n",
        "      else:\n",
        "        Xtrain = self.append_to_2d(Xtrain, Xcase)\n",
        "        Ytrain = self.append_to_2d(Ytrain, Ycase)\n",
        "      counter = counter + 1\n",
        "    return np.array(Xtrain), np.array(Xtest), np.array(Ytrain), np.array(Ytest)\n",
        "\n",
        "  def tokenize(self, data, tokenizer):\n",
        "    if tokenizer is None:\n",
        "      tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "      tokenizer.fit_on_texts(data['X'])\n",
        "    X = tokenizer.texts_to_sequences(data['X'])\n",
        "    word_index = tokenizer.word_index\n",
        "    print(word_index)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    X = pad_sequences(X)\n",
        "    Y = pd.get_dummies(data['Y'])\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.tokenizer = tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LfY-qkVT1X"
      },
      "source": [
        "class helper:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def datasetListMergeMinus(self, dataset, subset):\n",
        "    wholeData = 1\n",
        "    for m in dataset:\n",
        "      if not m.equals(subset):\n",
        "        if type(wholeData) is int:\n",
        "          wholeData = m          \n",
        "        else:\n",
        "          wholeData = wholeData.append(m)\n",
        "    return wholeData\n",
        "  def multiDimStrToUpper(self, string):\n",
        "    nstring = []\n",
        "    for strns in string:\n",
        "      nstring.append([x.upper() for x in strns])\n",
        "    return nstring\n",
        "  def multiDimStrToLower(self, string):\n",
        "    nstring = []\n",
        "    for strns in string:\n",
        "      nstring.append([x.lower() for x in strns])\n",
        "    return nstring\n",
        "  def oneDimStrToLower(self, string):\n",
        "    nstring = []\n",
        "    for i in range(0, len(string)):\n",
        "      nstring.append(string[i].lower())\n",
        "    return nstring\n",
        "  def grabEventsFromHeader(self, header):\n",
        "    evs = []\n",
        "    c = 0\n",
        "    for ev in header:\n",
        "      if c > 0:\n",
        "        try:\n",
        "          num = int(ev)\n",
        "        except:\n",
        "          evs.append(ev)\n",
        "      c = c + 1\n",
        "    return evs\n",
        "\n",
        "  def rowIsFirst(self, row, activities, headers):\n",
        "    foundValues = []\n",
        "    for i in range(0,len(headers)):\n",
        "      val = headers[i]\n",
        "      if row[val] in activities:\n",
        "        foundValues.append(row[val])\n",
        "    if (len(foundValues) == 0):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def rowIsLast(self, row, evName):\n",
        "    rowEv = row.idxmax()\n",
        "    try:\n",
        "      if rowEv.lower() == evName.lower():\n",
        "        return True\n",
        "    except:\n",
        "      return False\n",
        "    return False\n",
        "\n",
        "  def divideMatrix(self, matrix):\n",
        "    headers = list(matrix.columns.values)\n",
        "    leftHeaders = []\n",
        "    rightHeaders = []\n",
        "    for i in range(0,len(headers)):\n",
        "      ev = headers[i]\n",
        "      try:\n",
        "          num = int(ev)        \n",
        "          leftHeaders.append(ev)        \n",
        "      except:\n",
        "          rightHeaders.append(ev)\n",
        "    leftData = matrix[leftHeaders]\n",
        "    rightData = matrix[rightHeaders]\n",
        "    return [leftData, rightData]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTeO1DxWVbA_"
      },
      "source": [
        "class training:\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    MAX_NB_WORDS =50   #50\n",
        "    EMBEDDING_DIM =50   #32\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    self.model = model\n",
        "  def train(self, tindx = '0'):\n",
        "    model = self.model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    print('Training...')\n",
        "    X = self.X\n",
        "    Y = self.Y\n",
        "    history = model.fit(X, Y,  epochs=50, batch_size=250, verbose=0)\n",
        "    self.model = model\n",
        "    model.save('Orig_model_'+tindx+'.h5')\n",
        "    return model.evaluate(X, Y)\n",
        "\n",
        "  def getModelFrom(self, modelprefix):\n",
        "    try:\n",
        "      modelName = 'Orig_model_'+modelprefix+'.h5';\n",
        "      self.model = load_model(modelName)\n",
        "    except:\n",
        "      self.train(modelprefix)\n",
        "\n",
        "  def align(self, from_, to_):\n",
        "    originalColumnNamesArr = to_.columns.values\n",
        "    driftedColumnNamesArr = from_.columns.values\n",
        "    colNum = []\n",
        "    for i in range(0, len(driftedColumnNamesArr)):\n",
        "      col = driftedColumnNamesArr[i]\n",
        "      if(col not in originalColumnNamesArr):\n",
        "        from_ = from_.drop(col, 1)\n",
        "        colNum.append(i)\n",
        "    for i in range(0, len(originalColumnNamesArr)):\n",
        "      col = originalColumnNamesArr[i]\n",
        "      if(col not in driftedColumnNamesArr):\n",
        "        from_[col] = 0\n",
        "    return from_, colNum\n",
        "\n",
        "  def validateModel(self, Prep_data, tokenizer, model,  X, Y):\n",
        "    predict_proba = model.predict(X)\n",
        "    colName = []\n",
        "    for i in Y:\n",
        "        colName.append(i)\n",
        "    dfObj = pd.DataFrame(list(np.round(predict_proba*100, decimals=0)), columns = colName)\n",
        "    Seq_Series=Prep_data.X.apply(pd.Series)\n",
        "    dfObj.reset_index(drop=True, inplace=True)\n",
        "    Seq_Series.reset_index(drop=True, inplace=True)\n",
        "    df_new = pd.concat([Seq_Series, dfObj], axis=1)\n",
        "    return df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_kWHt7DVjf2"
      },
      "source": [
        "class resultGraphing:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def decomposeResult(self, results):\n",
        "    helper_ = helper()\n",
        "    headers = list(results.columns.values)\n",
        "    events = helper_.grabEventsFromHeader(headers)\n",
        "    matrices = []\n",
        "    matricesLeft = []\n",
        "    lastIndex = 0\n",
        "    totalBegin = 0\n",
        "    ind = 0\n",
        "    newMatrix = helper_.divideMatrix(results)\n",
        "    for index, row in newMatrix[1].iterrows():\n",
        "      ind = index\n",
        "      rowIsLast = helper_.rowIsLast(row, \"end\")\n",
        "      if rowIsLast:\n",
        "        sequenceList = newMatrix[1].iloc[lastIndex:index+1, :]\n",
        "        sequenceListLeft = newMatrix[0].iloc[lastIndex:index+1, :]\n",
        "        matrices.append(sequenceList)\n",
        "        matricesLeft.append(sequenceListLeft)\n",
        "        lastIndex = index + 1\n",
        "    return matrices, matricesLeft\n",
        "\n",
        "  def rowIsAnewSequence(self, row):\n",
        "    try:\n",
        "      return math.isnan(row[\"1\"])\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  def linkAndProbabilities(self, matrices, count = 0):\n",
        "    links = []\n",
        "    probabilities = []\n",
        "    sequences = []\n",
        "    uniqueEvs = []\n",
        "    for i in range(0,len(matrices)):\n",
        "      thisMatrix = matrices[i]\n",
        "      lastEvent = \"Start\"\n",
        "      sequence = []\n",
        "      for index, row in thisMatrix.iterrows():\n",
        "        row = pd.to_numeric(row)\n",
        "        #print(row) # prints the rows\n",
        "        evName = row.idxmax(axis=1) # picks event with the highest probability\n",
        "        link = lastEvent + \"<-->\" + evName\n",
        "        sequence.append(evName)\n",
        "        if link not in links:\n",
        "          if lastEvent != evName:\n",
        "            if not (lastEvent.lower() == \"start\" and evName.lower() == \"end\"):\n",
        "              links.append(link)\n",
        "              prob = row[evName]\n",
        "              probabilities.append(prob)\n",
        "        lastEvent = evName\n",
        "      # The Last element is End and undesirable\n",
        "      sequence.pop()\n",
        "      sequences.append(sequence)\n",
        "    return links, probabilities, sequences\n",
        "\n",
        "  def linkAndProbabilities_2(self, seqs, probs):\n",
        "    links = []\n",
        "    probabilities = []\n",
        "    sequences = []\n",
        "    for i in range(0,len(probs)):\n",
        "      lastEvent = \"Start\"\n",
        "      sequence = []\n",
        "      considprob = probs[i]\n",
        "      cols = considprob.columns.values\n",
        "      for index, row in considprob.iterrows():\n",
        "        evName = row.idxmax()\n",
        "        link = lastEvent + \"<-->\" + evName\n",
        "        sequence.append(evName)\n",
        "        prob = considprob[evName].max(axis=0)\n",
        "        if link not in links:\n",
        "          links.append(link)            \n",
        "          probabilities.append(prob)              \n",
        "        lastEvent = evName\n",
        "      sequence.pop() #\n",
        "      sequences.append(sequence)\n",
        "    return links, probabilities, sequences\n",
        "\n",
        "  def linkAndProbabilities_3(self, seqs, probs):\n",
        "    links = []\n",
        "    probabilities = []\n",
        "    sequences = []\n",
        "    uniqueEvs = []\n",
        "    orphanLog = []\n",
        "    pointedToLog = []\n",
        "    allowedActivities = probs[0].columns.values\n",
        "    for i in range(0,len(seqs)):\n",
        "      thisMatrix = probs[i]\n",
        "      lastEvent = \"Start\"\n",
        "      sequence = []\n",
        "      completeSequence = seqs[i].iloc[-1]\n",
        "      for j in range(1, len(completeSequence)):\n",
        "        if completeSequence[j] in allowedActivities:\n",
        "          evName = completeSequence[j]\n",
        "          sequence.append(evName)\n",
        "          link = lastEvent + \"<-->\" + evName\n",
        "          prob = probs[i][evName].max(axis=0)\n",
        "          if link not in links:\n",
        "            links.append(link)\n",
        "            probabilities.append(prob)\n",
        "          lastEvent = evName\n",
        "        else:\n",
        "          link = lastEvent + \"<-->END\"\n",
        "          prob = 100\n",
        "          if link not in links:\n",
        "            links.append(link)            \n",
        "            probabilities.append(prob)              \n",
        "          lastEvent = \"END\"\n",
        "          break\n",
        "      sequences.append(sequence)\n",
        "    return links, probabilities, sequences\n",
        " \n",
        "  def drawGraph(self, transitions, probabilities, fileName = ''):\n",
        "    G = Digraph('process_model', filename=fileName+'.gv')\n",
        "    G.attr(rankdir='TB', size='8,6')\n",
        "    G.attr('node', shape='doublecircle', style=\"filled\", fillcolor=\"grey\")\n",
        "    G.node('Start')\n",
        "    G.node('END')\n",
        "    G.attr('node', shape='box', style=\"bold\")\n",
        "    for i in range(0,len(transitions)):\n",
        "      G.attr('edge', style=\"bold\", penwidth='3.0', label=str(probabilities[i]))\n",
        "      fromto = transitions[i].split(\"<-->\")\n",
        "      G.edge(fromto[0], fromto[1])\n",
        "    G.view()\n",
        "    return G\n",
        "  \n",
        "  def getEventSequence(self, data, X_label, Y_label):\n",
        "    currentX_label = ''\n",
        "    sequences = []\n",
        "    sequence = []\n",
        "    for index, row in data.iterrows():\n",
        "      if currentX_label == row[X_label]:\n",
        "        sequence.append(row[Y_label])\n",
        "      else:\n",
        "        if len(sequence) > 1:\n",
        "          sequences.append(sequence)\n",
        "        sequence = []\n",
        "        sequence.append(row[Y_label])\n",
        "      currentX_label = row[X_label]\n",
        "    sequences.append(sequence)\n",
        "    return sequences  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-oDgrnWdOT"
      },
      "source": [
        "class performance:\n",
        "  def __init__(self):\n",
        "    self.i= 0\n",
        "  def getM(self, a, b):\n",
        "    M = 0\n",
        "    _index = 0\n",
        "    for index, row in a.iterrows():\n",
        "      columnVal = 0\n",
        "      for cell in row:\n",
        "        lCell = b.iloc[_index][columnVal]\n",
        "        if(lCell > 0 or cell > 0):\n",
        "          M = M + 1\n",
        "        columnVal = columnVal + 1\n",
        "      _index = _index + 1\n",
        "    return M\n",
        "\n",
        "  def getMatrix(self, chainTable, uniqueEV, testEvents):\n",
        "    global dfObjio\n",
        "    event_size = len(uniqueEV)\n",
        "    matrix = np.zeros((event_size, event_size))\n",
        "    counter = 0\n",
        "    for strings in chainTable[0]:\n",
        "      evs = strings.split(\"<-->\")\n",
        "      if \"Start\" in evs:\n",
        "        continue\n",
        "      if testEvents is None or (evs[0] in testEvents and evs[1] in testEvents):\n",
        "        probability = chainTable[1][counter]\n",
        "        if probability > 1:\n",
        "          probability = probability/100\n",
        "        col = uniqueEV.index(evs[0])\n",
        "        rw = uniqueEV.index(evs[1])\n",
        "        # if probability < threshold:\n",
        "        #   probability = 0\n",
        "        matrix[col, rw] = probability      \n",
        "        dfObjio = pd.DataFrame(matrix, index=uniqueEV, columns = uniqueEV)\n",
        "      counter = counter + 1\n",
        "    return dfObjio\n",
        "      \n",
        "      \n",
        "  def getAdjacency(self, matrixOne, matrixTwo, eventCount):  \n",
        "    M = self.getM(matrixOne, matrixTwo)\n",
        "    #case 1 Probability\n",
        "    absoluteVal = matrixOne.subtract(matrixTwo)\n",
        "    sumVal = absoluteVal.abs().sum().sum()\n",
        "    #case 2 Logicals\n",
        "    matrixOnelg = matrixOne.apply(np.ceil)\n",
        "    matrixTwolg = matrixTwo.apply(np.ceil)\n",
        "    absoluteVal = matrixOnelg.subtract(matrixTwolg)\n",
        "    sumValLogical = absoluteVal.abs().sum().sum()\n",
        "    #Estimate adjacency\n",
        "    print(M)\n",
        "    print(sumVal)\n",
        "    print(sumValLogical)\n",
        "    a = 1- sumVal/M\n",
        "    b = 1- sumValLogical/M\n",
        "    return [a, b]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfj-84QQ8AWW"
      },
      "source": [
        "class DatasetDefinitions:\n",
        "  def getUnique(self, label, dataset):\n",
        "    chains = []\n",
        "    chainTag = []\n",
        "    lastActivity = None\n",
        "    lastCase = None\n",
        "    eventLabel = label.event\n",
        "    caseLabel = label.case\n",
        "    for index, row in dataset.iterrows():\n",
        "      if lastCase is None or lastCase != row[caseLabel]:\n",
        "        lastActivity = None\n",
        "      if lastActivity is None:\n",
        "        lastActivity = row[eventLabel]\n",
        "        lastCase = row[caseLabel]\n",
        "        continue\n",
        "      lastCase = row[caseLabel]\n",
        "      if lastActivity != row[eventLabel]:\n",
        "        evChain = lastActivity +\"\"+row[eventLabel]\n",
        "        evChain = evChain.lower().strip()\n",
        "        evChain = \" \".join(evChain.split()).replace(' ', '_')\n",
        "        if evChain not in chains:\n",
        "          chains.append(evChain)\n",
        "          chainTag.append(lastCase)\n",
        "      lastActivity = row[eventLabel]\n",
        "    return chains, chainTag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIvcgnWU9g5P"
      },
      "source": [
        "class FindDrift:\n",
        "  def __init__(self, baseModel):\n",
        "    self.baseModel = baseModel\n",
        "  def executeAgainst(self, dataset, label):\n",
        "    _datasetDefinitions = DatasetDefinitions()\n",
        "    chain_base, tag_base = _datasetDefinitions.getUnique(label, self.baseModel)\n",
        "    chain, tag = _datasetDefinitions.getUnique(label, dataset)\n",
        "    indx = self.getDrifts(chain_base, chain)\n",
        "    tag_list = [tag[i] for i in indx]\n",
        "    chain_list = [chain[i] for i in indx]\n",
        "    return tag_list, chain_list, indx\n",
        "  def getDrifts(self, chain_base, chain_drift):\n",
        "    indx = []\n",
        "    counter = 0\n",
        "    for c in chain_drift:\n",
        "      if(c not in chain_base):\n",
        "        indx.append(counter)\n",
        "      counter = counter + 1\n",
        "    return indx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd6SRiRQWexE"
      },
      "source": [
        "labels = namedtuple(\"labels\", \"case event\")\n",
        "label = labels('case', 'event')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgRoSu7tWssb"
      },
      "source": [
        "parts = 10\n",
        "dataset = pd.read_csv('rp-2500.csv', low_memory= False)\n",
        "data_divider = dataDivider(dataset)\n",
        "p_ = data_divider.setParts(label, parts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlBRRp2Ilw4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeca53d9-44d8-4453-9901-d5a1ccd45160"
      },
      "source": [
        "print(data_divider.tracesPerPart)\n",
        "print(data_divider.totalCount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250\n",
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HZ0FmTWBGsB"
      },
      "source": [
        "adjacentScores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "Tcmu7uo_NziL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz31Y_megNJB"
      },
      "source": [
        "class windowManager:\n",
        "  def trainWindow(self, parts):\n",
        "    for i in range(0, parts):\n",
        "      j = i + 1\n",
        "      if(i == parts - 1):\n",
        "        j = 0\n",
        "      #Get the log part to be used\n",
        "      referenceLog = data_divider.getPart(i)\n",
        "      detectionLog = data_divider.getPart(j)\n",
        "      referenceLog.to_csv('referenceLogRP_'+str(i)+'.csv')\n",
        "\n",
        "      #Prepare the data Reference\n",
        "      prepdata = prepareData(referenceLog, ['case', 'event'])\n",
        "      X_train, X_test, Y_train, Y_test = prepdata.prepare(None, 0)\n",
        "\n",
        "      #get built tokenizer and word_index\n",
        "      tokenizer = prepdata.tokenizer\n",
        "      X = prepdata.X\n",
        "      Y = prepdata.Y\n",
        "      X_O = X\n",
        "      Y_O = Y\n",
        "      v = tokenizer.word_index.keys()\n",
        "\n",
        "      #Prepare the data Detection\n",
        "      prepdata_t = prepareData(detectionLog, ['case', 'event'])\n",
        "      X_train_t, X_test_t, Y_train_t, Y_test_t = prepdata_t.prepare(list(v), 0, tokenizer) #list(v)  \n",
        "      tokenizer_t = prepdata_t.tokenizer\n",
        "      X_t = prepdata_t.X\n",
        "      Y_t = prepdata_t.Y\n",
        "      X_T = X_t\n",
        "      Y_T = Y_t\n",
        "\n",
        "\n",
        "      r = str(i)+\"_0\"\n",
        "      r2 = str(i)+\"_1\"\n",
        "      #Train the Models\n",
        "      trainModel_O = training(X_train, Y_train)\n",
        "      trainModel_O.getModelFrom(r)\n",
        "\n",
        "      trainModel_T = training(X_train_t, Y_train_t)\n",
        "      trainModel_T.getModelFrom(r2)\n",
        "      \n",
        "      #Use on their dataset\n",
        "      resultDataset_O = trainModel_O.validateModel(prepdata.maindfObj, tokenizer, trainModel_O.model,  X, Y)\n",
        "      resultDataset_T = trainModel_T.validateModel(prepdata_t.maindfObj, tokenizer_t, trainModel_T.model,  X_t, Y_t)\n",
        "      resultDataset_O.to_csv('detectionLogRP_'+r+'.csv')\n",
        "      #resultDataset_T.to_csv('referenceLog_'+r2+'.csv')\n",
        "\n",
        "\n",
        "      #Result graphing\n",
        "      resultGraphing_ = resultGraphing()\n",
        "      #Reference\n",
        "      probs_O, seqs_O = resultGraphing_.decomposeResult(resultDataset_O)\n",
        "      link_O, probabilities_O, sequences_O = resultGraphing_.linkAndProbabilities_3(seqs_O, probs_O)\n",
        "      resultGraphing_.drawGraph(link_O, probabilities_O, r)\n",
        "      #Adjc log\n",
        "      probs_T, seqs_T = resultGraphing_.decomposeResult(resultDataset_T)\n",
        "      link_T, probabilities_T, sequences_T = resultGraphing_.linkAndProbabilities_3(seqs_O, probs_T)\n",
        "      resultGraphing_.drawGraph(link_T, probabilities_T, r2)\n",
        "\n",
        "      perf = performance()\n",
        "       \n",
        "      h1 = h2 = None\n",
        "      if len(Y_O.columns.values) > len(Y_T.columns.values): \n",
        "        events = list(Y_T.columns.values)\n",
        "        h1 = perf.getMatrix([link_T, probabilities_T], events, None)\n",
        "        h2 = perf.getMatrix([link_O, probabilities_O], list(Y_O.columns.values), events)\n",
        "      else:\n",
        "        events = list(Y_O.columns.values)\n",
        "        h1 = perf.getMatrix([link_O, probabilities_O], events, None)\n",
        "        h2 = perf.getMatrix([link_T, probabilities_T], list(Y_T.columns.values), events)\n",
        "      result = perf.getAdjacency(h1, h2, len(events))\n",
        "      ra = (i, j, result)\n",
        "      adjacentScores.append(ra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBR3ba2ukkvU"
      },
      "source": [
        "_windowManager = windowManager()\n",
        "_windowManager.trainWindow(parts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cAxZxE3YN7n"
      },
      "source": [
        "adjacentScores"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}